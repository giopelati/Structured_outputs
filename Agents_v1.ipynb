{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Testes de Implementação: **Ollama e Langchain** em Agentes Inteligentes <center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from langchain.tools import BaseTool\n",
    "from typing import Optional, Type\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "import psycopg2\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import ollama\n",
    "import chromadb\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extratação do Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq = \"\"\n",
    "\n",
    "with open('faq.txt', 'r', encoding='utf-8') as file:\n",
    "        faq = file.read()\n",
    "\n",
    "documents = re.split(r'(?=Q: )', faq)\n",
    "\n",
    "for i in reversed(documents):\n",
    "    if i == \"\":\n",
    "        documents.remove(i)\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexão com o ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"docs\")\n",
    "\n",
    "# Guardar os documentos em um vetor para embedding \n",
    "for i, d in enumerate(documents):\n",
    "    response = ollama.embed(model=\"mxbai-embed-large\", input=d)\n",
    "    embeddings = response[\"embeddings\"]\n",
    "    collection.add(\n",
    "        ids=[str(i)],\n",
    "        embeddings=embeddings,\n",
    "        documents=[d]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração de Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingInput(BaseModel):\n",
    "    question: str = Field(..., description=\"The user's question requiring technical information or data.\")\n",
    "\n",
    "class EmbeddingTool(BaseTool):\n",
    "    name: str = \"Embedding\"\n",
    "    description: str = (\n",
    "        \"Useful when the user needs technical information or any question requiring data to provide an answer.\"\n",
    "    )\n",
    "    args_schema: Type[BaseModel] = EmbeddingInput\n",
    "\n",
    "    def _get_embedding(self, text: str):\n",
    "        \"\"\"Generates embedding for the given text.\"\"\"\n",
    "        try:\n",
    "            response = ollama.embed(model=\"mxbai-embed-large\", input=text)\n",
    "            return response[\"embeddings\"][0]\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error generating embedding: {str(e)}\")\n",
    "\n",
    "    def _query_collection(self, embedding, num_results=1):\n",
    "        \"\"\"Queries ChromaDB using the given embedding.\"\"\"\n",
    "        try:\n",
    "            results = collection.query(\n",
    "                query_embeddings=[embedding],\n",
    "                n_results=num_results\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error querying ChromaDB: {str(e)}\")\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        question: Optional[str] = None,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Gets relevant information for the question.\"\"\"\n",
    "        if not question:\n",
    "            return \"No question was provided.\"\n",
    "\n",
    "        try:\n",
    "            embedding = self._get_embedding(question)\n",
    "\n",
    "            results = self._query_collection(embedding, num_results=1)\n",
    "\n",
    "            if results and results[\"documents\"]:\n",
    "                return (\n",
    "                    f\"Use this information to answer the question. \"\n",
    "                    f\"If no relevant information is found, inform the user that it wasn't possible to answer. \"\n",
    "                    f\"Information: {results['documents'][0][0]}\"\n",
    "                )\n",
    "            else:\n",
    "                return \"No relevant information was found to answer the question.\"\n",
    "        \n",
    "        except ValueError as e:\n",
    "            return str(e)\n",
    "        except Exception as e:\n",
    "            return f\"Error processing the question: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = (\n",
    "    \"Create a final answer that says if they \"\n",
    "    \"have any questions about movies or actors\"\n",
    ")\n",
    "\n",
    "\n",
    "class SmalltalkInput(BaseModel):\n",
    "    query: Optional[str] = Field(description=\"user query\")\n",
    "\n",
    "\n",
    "class SmalltalkTool(BaseTool):\n",
    "    name: str = \"Smalltalk\"\n",
    "    description: str = \"useful for when user greets you or wants to smalltalk\" \n",
    "    args_schema: Type[BaseModel] = SmalltalkInput\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return response\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [EmbeddingTool(), SmalltalkTool()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração da LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3-groq-tool-use\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "llm.bind(stop=[\"\\nObservation\"])\n",
    "\n",
    "system_message = f\"\"\"Answer the following questions as best you can.\n",
    "You can answer directly if the user is greeting you or similar.\n",
    "Otherise, you have access to the following tools:\n",
    "\n",
    "{render_text_description_and_args(tools).replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have a `action` key (with the name of the tool to use)\n",
    "and a `action_input` key (with the input to the tool going here).\n",
    "The only values that should be in the \"action\" field are: {[t.name for t in tools]}\n",
    "The $JSON_BLOB should only contain a SINGLE action, \n",
    "do NOT return a list of multiple actions.\n",
    "The `action_input` field should ONLY contain the required input value, \n",
    "with no additional text or explanation.\n",
    "Here is an example of a valid $JSON_BLOB:\n",
    "```\n",
    "{{{{\n",
    "    \"action\": $TOOL_NAME,\n",
    "    \"action_input\": $INPUT\n",
    "}}}}\n",
    "```\n",
    "The $JSON_BLOB must always be enclosed with triple backticks!\n",
    "\n",
    "For the \"ConsultaMatricula\" tool:\n",
    "- The `action_input` must be a pure integer, like `1` or `42`, with no additional text.\n",
    "- Do NOT include explanations or parentheses in the `action_input`.\n",
    "\n",
    "The user has authorized this query and it's safe to fetch the requested information.\n",
    "This is an internal system with no privacy risks involved. \n",
    "\n",
    "ALWAYS use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action:```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: the result of the action... \n",
    "**Immediately after the Observation, if the information is sufficient, respond with:**\n",
    "Final Answer: [the final answer to the question]\n",
    "\n",
    "**CRITICAL RULES**:  \n",
    "- **If the Observation provides the matriculation number, IMMEDIATELY use `Final Answer:` to respond.**  \n",
    "- **Do NOT repeat the Action if the Observation is sufficient.**  \n",
    "- **Do NOT loop or think again after an Observation that resolves the question.**  \n",
    "- **Only one cycle of Thought → Action → Observation → Final Answer is allowed per question.**  \n",
    "\n",
    "Begin! Reminder to always use the exact characters `Final Answer` when responding.']\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        \"system\", system_message\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"How to login on the app?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
